{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cc87df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: core2, Number of .dot files: 429\n",
      "Category: core5, Number of .dot files: 442\n",
      "Category: core0, Number of .dot files: 433\n",
      "Category: core1, Number of .dot files: 433\n",
      "Category: core4, Number of .dot files: 434\n",
      "Category: core3, Number of .dot files: 435\n",
      "\n",
      "====== Training ======\n",
      "\n",
      "Total number of campaigns in the training dataset: 2171\n",
      "\n",
      "Plotting all different patterns in the Training set: 28\n",
      "\n",
      "Weird Machine classifications in Training: 28\n",
      "Pattern 2dd3fd3b2c7df350241568aaf9797189: Wormhole (1013 samples)\n",
      "Pattern 184a37a7bc6649027d7bc630f251903f: Wormhole (243 samples)\n",
      "Pattern 3e8ac51ed1241beac4ad7b88af311791: Tunnel (237 samples)\n",
      "Pattern e4a9c4ac4b84be7668569728c8f105ad: Wormhole (190 samples)\n",
      "Pattern a6f2e5ee1911c56a1599d39c521ac1bb: Wormhole (181 samples)\n",
      "Pattern 164ca248e5cccd024c607aadacebcb85: Wormhole (18 samples)\n",
      "Pattern 0bc05b01d66b72cc33afc1b94ec6b455: Tunnel (162 samples)\n",
      "Pattern d8cd5cf7be398bf6ec514523b180d5ce: Wormhole (6 samples)\n",
      "Pattern ca7b63a3dfb98cfe8fdf6dc50174830f: Spinner (26 samples)\n",
      "Pattern b03c1d546cbc6ed3c88b8a5bbd34ed7d: Unknown (11 samples)\n",
      "Pattern 8e0df7746bea792a82e3ef7495e240d7: Tunnel (29 samples)\n",
      "Pattern 47c9a74cdb0856de0eb852669df33f4b: Wormhole (7 samples)\n",
      "Pattern 470d3fddd532ac2834d4f35750e68fcd: Tunnel (8 samples)\n",
      "Pattern aed40701e3fb5c960dc12c581fd9de1a: Spinner (13 samples)\n",
      "Pattern f483a63754d1caa94fc541eaf29d09a2: Wormhole (1 samples)\n",
      "Pattern 2fea8ba647832c23794f66fbba65b879: Wormhole (9 samples)\n",
      "Pattern 975df8ba6889b5d07d6a9e41b47b74cb: Wormhole (1 samples)\n",
      "Pattern 8d99dfb71b084873ce2a370c332e6f8d: Wormhole (1 samples)\n",
      "Pattern 72e9a86ce6a5ab96d1309d0d4a047879: Wormhole (1 samples)\n",
      "Pattern 8c72587aadaff95352e799d1632ee0d5: Wormhole (1 samples)\n",
      "Pattern b4b99b85f8ce7789d93465837604ab72: Tunnel (2 samples)\n",
      "Pattern 04520a5eef89f66e50b9002385ab47fb: Wormhole (5 samples)\n",
      "Pattern f8b1d947d4e38913ff27924d7f0d6965: Wormhole (1 samples)\n",
      "Pattern 0140ea5106174cdd373a55f1aee7caf0: Spinner (1 samples)\n",
      "Pattern e6df0e70ed55b1141dd2b16a80dc3ea4: Spinner (1 samples)\n",
      "Pattern cb808e05cc36fb995b5b8c3e3decd7b7: Spinner (1 samples)\n",
      "Pattern 805ff11044999554fa01f4db912c91f4: Wormhole (1 samples)\n",
      "Pattern cc98c2a7106c4cf2194ab38842f66142: Wormhole (1 samples)\n",
      "\n",
      "====== Testing ======\n",
      "\n",
      "Total number of campaigns in the testing dataset: 435\n",
      "\n",
      "Plotting all different patterns in the Testing set: 16\n",
      "\n",
      "Weird machine classifications in Testing: 16\n",
      "Pattern a6f2e5ee1911c56a1599d39c521ac1bb: Wormhole (33 samples)\n",
      "Pattern 2dd3fd3b2c7df350241568aaf9797189: Wormhole (191 samples)\n",
      "Pattern e4a9c4ac4b84be7668569728c8f105ad: Wormhole (32 samples)\n",
      "Pattern 164ca248e5cccd024c607aadacebcb85: Wormhole (6 samples)\n",
      "Pattern 0bc05b01d66b72cc33afc1b94ec6b455: Tunnel (51 samples)\n",
      "Pattern 184a37a7bc6649027d7bc630f251903f: Wormhole (45 samples)\n",
      "Pattern 3e8ac51ed1241beac4ad7b88af311791: Tunnel (49 samples)\n",
      "Pattern 04520a5eef89f66e50b9002385ab47fb: Wormhole (5 samples)\n",
      "Pattern 2fea8ba647832c23794f66fbba65b879: Wormhole (1 samples)\n",
      "Pattern ca7b63a3dfb98cfe8fdf6dc50174830f: Spinner (3 samples)\n",
      "Pattern cb808e05cc36fb995b5b8c3e3decd7b7: Spinner (1 samples)\n",
      "Pattern 8e0df7746bea792a82e3ef7495e240d7: Tunnel (8 samples)\n",
      "Pattern d8cd5cf7be398bf6ec514523b180d5ce: Wormhole (1 samples)\n",
      "Pattern aed40701e3fb5c960dc12c581fd9de1a: Spinner (4 samples)\n",
      "Pattern b03c1d546cbc6ed3c88b8a5bbd34ed7d: Wormhole (4 samples)\n",
      "Pattern 80c7b416be7816b5d19bbc6fc64382f3: Wormhole (1 samples)\n",
      "=========== Result =========\n",
      "\n",
      "Number of weird machine patterns in the testing dataset already seen in the training dataset: 15\n",
      "Number of weird machine patterns in the testing dataset not seen in the training dataset: 1\n",
      "New weird machine pattern(s) in the testing dataset: ['80c7b416be7816b5d19bbc6fc64382f3']\n",
      "New weird machine pattern(s) in the testing dataset dot file(s): ['/media/dillibabu/PortableSSD/Exp/capri6_rootcause_data/lasersweep/analysis/cores/date24/patternmatch_complete/testing_dotfiles/tb-2848_2146_core3_uarchi_abstract.dot']\n"
     ]
    }
   ],
   "source": [
    "#import torch\n",
    "#import torch.nn.functional as F\n",
    "#from torch_geometric.nn import GATConv, SAGEConv\n",
    "#from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "import pygraphviz as pgv\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from hashlib import md5\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from networkx.algorithms.isomorphism import GraphMatcher\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import shutil\n",
    "# Getting the current date and time\n",
    "dt = datetime.now()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    return ''.join(BeautifulSoup(text, \"html.parser\").stripped_strings)\n",
    "\n",
    "# Function to convert clock cycle labels (e.g., \"@21+41k\" to 62)\n",
    "def convert_clock_cycle(label):\n",
    "    cleaned_label = strip_html_tags(label)\n",
    "    match = re.match(r'@(\\d+)(\\+(\\d+)k)?', cleaned_label)\n",
    "    if match:\n",
    "        base = int(match.group(1))\n",
    "        increment = int(match.group(3)) * 1 if match.group(3) else 0\n",
    "        return base + increment\n",
    "    return 0\n",
    "\n",
    "# Function to parse .dot file and prepare the graph\n",
    "def parse_dot_file(dot_path):\n",
    "    A = pgv.AGraph(file=dot_path)\n",
    "    G = nx.DiGraph(A)\n",
    "    \n",
    "    for u, v, data in G.edges(data=True):\n",
    "        if 'label' in data:\n",
    "            data['label'] = convert_clock_cycle(data['label'])\n",
    "    \n",
    "    return G\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    # Replace invalid characters with underscores\n",
    "    return re.sub(r'[<>:\"/\\\\|?*]', '_', filename)\n",
    "\n",
    "# Function to hash a knowledge graph to identify unique patterns\n",
    "def hash_graph(G):\n",
    "    graph_str = nx.weisfeiler_lehman_graph_hash(G)\n",
    "    return md5(graph_str.encode('utf-8')).hexdigest()\n",
    "\n",
    "def plot_graph(G, title, highlight_edges=None):\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    nx.draw(G, pos, with_labels=True, node_size=500, node_color='lightblue', edge_color='gray', font_size=12, font_weight='bold')\n",
    "    \n",
    "    # Adding edge labels\n",
    "    edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='red', label_pos=0.3, bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))\n",
    "    \n",
    "    # Highlight the edges/nodes in red if they are in the highlight list\n",
    "    if highlight_edges:\n",
    "        nx.draw_networkx_edges(G, pos, edgelist=highlight_edges, edge_color='red', width=2.5)\n",
    "\n",
    "    plt.title(title)\n",
    "    \n",
    "    # Sanitize title to create a valid filename\n",
    "    safe_title = sanitize_filename(title)\n",
    "    numeric_plot_filename = f'{safe_title}.png'\n",
    "    \n",
    "    plt.savefig(numeric_plot_filename)\n",
    "    plt.close()\n",
    "# Function to create a knowledge graph representation\n",
    "def create_knowledge_graph(G):\n",
    "    knowledge_graph = nx.Graph()\n",
    "\n",
    "    # Add nodes with features as attributes\n",
    "    for node in G.nodes():\n",
    "        knowledge_graph.add_node(node, features=G.nodes[node])\n",
    "\n",
    "    # Add edges with attributes\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        knowledge_graph.add_edge(u, v, **data)\n",
    "\n",
    "    return knowledge_graph\n",
    "\n",
    "# Function to classify a pattern based on the rules provided\n",
    "def classify_pattern(G):\n",
    "    first_five_nodes = set([f\"M{i}\" for i in range(4)])  # Assuming nodes are named like M0, M1, M2, M3,\n",
    "    long_jump_found = False\n",
    "    long_jump_node = None\n",
    "    pattern_type = \"Unknown\"\n",
    "\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        try:\n",
    "            u_index = int(re.findall(r'\\d+', u)[0])\n",
    "            v_index = int(re.findall(r'\\d+', v)[0])\n",
    "        except (IndexError, ValueError):\n",
    "            continue\n",
    "\n",
    "        if abs(u_index - v_index) >= 2:\n",
    "            long_jump_found = True\n",
    "            long_jump_node = v\n",
    "            break\n",
    "\n",
    "    if long_jump_found:\n",
    "        for node in nx.dfs_postorder_nodes(G, source=long_jump_node):\n",
    "            if G.has_edge(node, node):\n",
    "                pattern_type = \"Spinner\"\n",
    "                break\n",
    "        else:\n",
    "            if long_jump_node in first_five_nodes:\n",
    "                connects_back = False\n",
    "                for node in nx.dfs_postorder_nodes(G, source=long_jump_node):\n",
    "                    for u, v in G.edges(data=False):\n",
    "                        if u == node and v in first_five_nodes:\n",
    "                            connects_back = True\n",
    "                            break\n",
    "                    if connects_back:\n",
    "                        break\n",
    "                if not connects_back:\n",
    "                    pattern_type = \"Tunnel and Wormhole\"\n",
    "                else:\n",
    "                    pattern_type = \"Unknown\"\n",
    "            else:\n",
    "                pattern_type = \"Wormhole\"\n",
    "    else:\n",
    "        for u, v, data in G.edges(data=True):\n",
    "            if u in first_five_nodes and v in first_five_nodes:\n",
    "                pattern_type = \"Tunnel\"\n",
    "                break\n",
    "\n",
    "    return pattern_type\n",
    "\n",
    "# Base folder path\n",
    "#base_folder = \"/mnt/labdrive/zliu12/capri6_rootcause_data/lasersweep/lasersweep_asconsbox_bitslice/\"\n",
    "base_folder = \"/media/dillibabu/PortableSSD/Exp/capri6_rootcause_data/lasersweep/analysis/cores/date24/patternmatch_complete/\"\n",
    "training_dotfiles_folder_path = os.path.join(base_folder, 'training_dotfiles') \n",
    "testing_dotfiles_folder_path = os.path.join(base_folder, 'testing_dotfiles') \n",
    "# Function to strip HTML tags from labels (if present)\n",
    "def strip_html_tags(text):\n",
    "    return ''.join(BeautifulSoup(text, \"html.parser\").stripped_strings)\n",
    "\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "core0=[]\n",
    "core1=[]\n",
    "core2=[]\n",
    "core3=[]\n",
    "core4=[]\n",
    "core5=[]\n",
    "\n",
    "\n",
    "\n",
    "def gather_and_classify_training_dot_files(training_dotfiles_folder_path, exclude_folders=['coremajority']):\n",
    "    dot_files = defaultdict(list)  # To store files by categories\n",
    "    categories_count = defaultdict(int)  # To count files in each category\n",
    "    training_dotfiles= []\n",
    "\n",
    "    for dot_file in os.listdir(training_dotfiles_folder_path):\n",
    "        if dot_file.endswith('uarchi_abstract.dot') and not any(exclude in dot_file for exclude in exclude_folders) and 'core3' not in dot_file:\n",
    "            training_dotfiles.append(os.path.join(training_dotfiles_folder_path, dot_file))\n",
    "#                         print('dot_file_training:',dot_file)\n",
    "\n",
    "            category = None\n",
    "            if 'core0' in dot_file:\n",
    "                category = 'core0'\n",
    "                core0.append(dot_file)\n",
    "            elif 'core1' in dot_file:\n",
    "                category = 'core1'\n",
    "                core1.append(dot_file)\n",
    "            elif 'core2' in dot_file:\n",
    "                category = 'core2'\n",
    "                core2.append(dot_file)\n",
    "            elif 'core3' in dot_file:\n",
    "                category = 'core3'\n",
    "                core3.append(dot_file)\n",
    "            elif 'core4' in dot_file:\n",
    "                category = 'core4'\n",
    "                core4.append(dot_file)\n",
    "            elif 'core5' in dot_file:\n",
    "                category = 'core5'\n",
    "                core5.append(dot_file)\n",
    "\n",
    "            # Add more elif cases for other cores if needed\n",
    "\n",
    "            if category:\n",
    "                dot_files[category].append(os.path.join(training_dotfiles_folder_path, dot_file))\n",
    "                categories_count[category] += 1\n",
    "                    \n",
    "                    \n",
    "\n",
    "    # Print the total number of .dot files in each category\n",
    "    for category, files in dot_files.items():\n",
    "        print(f'Category: {category}, Number of .dot files: {categories_count[category]}')\n",
    "#     print('core0',core0)\n",
    "#     print('core1',core1)\n",
    "#     print('core2',core2)\n",
    "#     print('core4',core4)\n",
    "#     print('core5',core5)\n",
    "\n",
    "    return training_dotfiles,dot_files, categories_count\n",
    "\n",
    "\n",
    "def gather_and_classify_testing_dot_files(testing_dotfiles_folder_path, exclude_folders=['coremajority']):\n",
    "    dot_files = defaultdict(list)  # To store files by categories\n",
    "    categories_count = defaultdict(int)  # To count files in each category\n",
    "    testing_dotfiles=[]\n",
    "    \n",
    "    for dot_file in os.listdir(testing_dotfiles_folder_path):\n",
    "        if dot_file.endswith('uarchi_abstract.dot') and not any(exclude in dot_file for exclude in exclude_folders) and 'core3' in dot_file:\n",
    "            testing_dotfiles.append(os.path.join(testing_dotfiles_folder_path, dot_file))\n",
    "#                         print('dot_file_training:',dot_file)\n",
    "\n",
    "            category = None\n",
    "            if 'core3' in dot_file:\n",
    "                category = 'core3'\n",
    "                core3.append(dot_file)\n",
    "            if category:\n",
    "                dot_files[category].append(os.path.join(testing_dotfiles_folder_path, dot_file))\n",
    "                categories_count[category] += 1\n",
    "                    \n",
    "    # Print the total number of .dot files in each category\n",
    "    for category, files in dot_files.items():\n",
    "        print(f'Category: {category}, Number of .dot files: {categories_count[category]}')\n",
    "#     print('core3',core3)\n",
    "    \n",
    "\n",
    "    return testing_dotfiles,dot_files, categories_count\n",
    "\n",
    "\n",
    "training_dot_file_paths,  category_type, categories_count = gather_and_classify_training_dot_files(training_dotfiles_folder_path)\n",
    "\n",
    "testing_dot_file_paths, category_type, categories_count = gather_and_classify_testing_dot_files(testing_dotfiles_folder_path)\n",
    "#-----------------------\n",
    "\n",
    "# Check if tb_13cc_dot_file_paths is empty\n",
    "if not testing_dot_file_paths:\n",
    "    print(\"No .dot files found in the testing directory. Please check the directory path.\")\n",
    "else:\n",
    "    # Load the list from the file\n",
    "    #with open('train_dot_file_paths.pkl', 'rb') as file:\n",
    "        #train_dot_file_paths = pickle.load(file)\n",
    "    # Split tb-13cc data into training and testing sets\n",
    "    tb_cc_test_files = testing_dot_file_paths\n",
    "\n",
    "    # Parse and prepare training knowledge graphs and collect patterns\n",
    "    train_patterns = {}\n",
    "    pattern_count_train = {}\n",
    "    pattern_graphs_train = {}\n",
    "    pattern_classification_train = {}\n",
    "    for path in training_dot_file_paths:\n",
    "        train_G = parse_dot_file(path)\n",
    "        train_kg = create_knowledge_graph(train_G)\n",
    "        pattern_hash = hash_graph(train_kg)\n",
    "        train_patterns[pattern_hash] = train_G\n",
    "        pattern_count_train[pattern_hash] = pattern_count_train.get(pattern_hash, 0) + 1\n",
    "        if pattern_hash not in pattern_graphs_train:\n",
    "            pattern_graphs_train[pattern_hash] = []\n",
    "        pattern_graphs_train[pattern_hash].append(train_G)\n",
    "        pattern_type = classify_pattern(train_G)\n",
    "        pattern_classification_train[pattern_hash] = pattern_type\n",
    "\n",
    "    # Parse and prepare testing knowledge graphs and collect patterns\n",
    "    test_patterns = {}\n",
    "    pattern_count_test = {}\n",
    "    pattern_graphs_test = {}\n",
    "    pattern_classification_test = {}\n",
    "    seen_in_training = 0\n",
    "    not_seen_in_training = 0\n",
    "    unknown_pattern_dot_files = []\n",
    "    known_pattern_dot_files = []\n",
    "    new_pattern_dot_files = []\n",
    "\n",
    "    for path in tb_cc_test_files:\n",
    "        test_G = parse_dot_file(path)\n",
    "        test_kg = create_knowledge_graph(test_G)\n",
    "        pattern_hash = hash_graph(test_kg)\n",
    "        test_patterns[pattern_hash] = test_G\n",
    "        pattern_count_test[pattern_hash] = pattern_count_test.get(pattern_hash, 0) + 1\n",
    "        if pattern_hash not in pattern_graphs_test:\n",
    "            pattern_graphs_test[pattern_hash] = []\n",
    "        pattern_graphs_test[pattern_hash].append(test_G)\n",
    "        pattern_type = classify_pattern(test_G)\n",
    "        pattern_classification_test[pattern_hash] = pattern_type\n",
    "        \n",
    "        if pattern_hash in train_patterns:\n",
    "            seen_in_training += 1\n",
    "            plot_graph(test_G, title=f\"Test Pattern Seen in Training: {pattern_hash}\", highlight_edges=list(test_G.edges()))\n",
    "        else:\n",
    "            not_seen_in_training += 1\n",
    "            plot_graph(test_G, title=f\"Test Pattern Not Seen in Training: {pattern_hash}\", highlight_edges=list(test_G.edges()))\n",
    "            new_pattern_dot_files.append(path)\n",
    "    \n",
    "    unknown_patterns = []\n",
    "    known_patterns = []\n",
    "    no_unknown_pattern = 0\n",
    "    no_known_pattern = 0\n",
    "\n",
    "    for pattern_hash, pattern_type in pattern_classification_test.items():\n",
    "        #print('pattern_hash', pattern_hash)\n",
    "        if pattern_hash not in pattern_classification_train:\n",
    "            unknown_patterns.append(pattern_hash)\n",
    "            no_unknown_pattern += 1\n",
    "        else:\n",
    "            known_patterns.append(pattern_hash)\n",
    "            no_known_pattern += 1\n",
    "        \n",
    "\n",
    "\n",
    "    total_train_samples = sum(pattern_count_train.values())\n",
    "    total_test_samples = sum(pattern_count_test.values())\n",
    "    \n",
    "    print(\"\\n====== Training ======\\n\")\n",
    "    print(f\"Total number of campaigns in the training dataset: {total_train_samples}\")\n",
    "    # Plot all different patterns found in the training set\n",
    "    print(\"\\nPlotting all different patterns in the Training set:\", len(pattern_classification_train.items()))\n",
    "    for pattern_hash, G in train_patterns.items():\n",
    "        plot_graph(G, title=f\"Training Pattern: {pattern_hash} ({pattern_classification_train[pattern_hash]})\")\n",
    "\n",
    "    # Output the classification results\n",
    "    print(\"\\nWeird Machine classifications in Training:\", len(pattern_classification_train.items()))\n",
    "    for pattern_hash, pattern_type in pattern_classification_train.items():\n",
    "        print(f\"Pattern {pattern_hash}: {pattern_type} ({pattern_count_train[pattern_hash]} samples)\")\n",
    "    print(\"\\n====== Testing ======\\n\")\n",
    "    print(f\"Total number of campaigns in the testing dataset: {total_test_samples}\")   \n",
    "    \n",
    "    \n",
    "    # Plot all different patterns found in the testing set\n",
    "    print(\"\\nPlotting all different patterns in the Testing set:\", len(pattern_classification_test.items()))\n",
    "    for pattern_hash, G in test_patterns.items():\n",
    "        plot_graph(G, title=f\"Testing Pattern: {pattern_hash} ({pattern_classification_test[pattern_hash]})\")\n",
    "\n",
    "    \n",
    "    print(\"\\nWeird machine classifications in Testing:\", len(pattern_classification_test.items()))\n",
    "    for pattern_hash, pattern_type in pattern_classification_test.items():\n",
    "        print(f\"Pattern {pattern_hash}: {pattern_type} ({pattern_count_test[pattern_hash]} samples)\")\n",
    "\n",
    "    print(\"=========== Result =========\\n\")\n",
    "    print(f\"Number of weird machine patterns in the testing dataset already seen in the training dataset: {no_known_pattern}\")\n",
    "    print(f\"Number of weird machine patterns in the testing dataset not seen in the training dataset: {no_unknown_pattern}\")\n",
    "    print(f\"New weird machine pattern(s) in the testing dataset: {unknown_patterns}\")\n",
    "    print(f\"New weird machine pattern(s) in the testing dataset dot file(s): {new_pattern_dot_files}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a6c5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cf0b92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
