{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a18b653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: core2, Number of .dot files: 429\n",
      "Category: core5, Number of .dot files: 442\n",
      "Category: core0, Number of .dot files: 433\n",
      "Category: core1, Number of .dot files: 433\n",
      "Category: core4, Number of .dot files: 434\n",
      "Category: core3, Number of .dot files: 435\n",
      "\n",
      "====== Training ======\n",
      "\n",
      "Total number of campaigns in the training dataset: 2171\n",
      "\n",
      "Plotting all different patterns in the Training set: 74\n",
      "\n",
      "Weird Machine classifications in Training: 74\n",
      "Pattern 2dd3fd3b2c7df350241568aaf9797189: Wormhole (881 samples)\n",
      "Pattern 184a37a7bc6649027d7bc630f251903f: Wormhole (243 samples)\n",
      "Pattern 3e8ac51ed1241beac4ad7b88af311791: Tunnel (331 samples)\n",
      "Pattern a6f2e5ee1911c56a1599d39c521ac1bb: Wormhole (186 samples)\n",
      "Pattern e4a9c4ac4b84be7668569728c8f105ad: Wormhole (127 samples)\n",
      "Pattern 24a21209ad3ad01e7e8177ff4b0ec23f: Wormhole (1 samples)\n",
      "Pattern 0bc05b01d66b72cc33afc1b94ec6b455: Tunnel (182 samples)\n",
      "Pattern 8e0df7746bea792a82e3ef7495e240d7: Tunnel (7 samples)\n",
      "Pattern 6622bc2e2e6203d8bce93101bcfa9c1d: Spinner (15 samples)\n",
      "Pattern 600719bc4cacda837446a6dbe80d0d63: Wormhole (4 samples)\n",
      "Pattern ca7b63a3dfb98cfe8fdf6dc50174830f: Spinner (1 samples)\n",
      "Pattern 7e252d378eba6eaaca3c3e4c0978e0cc: Wormhole (6 samples)\n",
      "Pattern 470d3fddd532ac2834d4f35750e68fcd: Tunnel (9 samples)\n",
      "Pattern 2fea8ba647832c23794f66fbba65b879: Wormhole (15 samples)\n",
      "Pattern 805ff11044999554fa01f4db912c91f4: Wormhole (3 samples)\n",
      "Pattern 04520a5eef89f66e50b9002385ab47fb: Wormhole (12 samples)\n",
      "Pattern 543f62ac8debd38af28338f861e23000: Wormhole (1 samples)\n",
      "Pattern 08e8c8ab9455520881ab410f98fbb854: Wormhole (27 samples)\n",
      "Pattern 790523bcdfc630dfc7f7a35273a8275b: Wormhole (8 samples)\n",
      "Pattern c9501919a7b13b006a0a0a634ebaf0b8: Spinner (1 samples)\n",
      "Pattern 95c43e6f52c21b01e9192991ad3b6544: Wormhole (4 samples)\n",
      "Pattern 3e619b6c5540ca1e9ce0686641558fd8: Wormhole (8 samples)\n",
      "Pattern 3e6cde3b31ebf61cc314c7a272175d82: Wormhole (1 samples)\n",
      "Pattern aed40701e3fb5c960dc12c581fd9de1a: Spinner (7 samples)\n",
      "Pattern d6e6bb9b0793787defe276d48d963d66: Wormhole (7 samples)\n",
      "Pattern d271b60396bad7e52aa166e8615bce64: Tunnel (3 samples)\n",
      "Pattern 6ad6e158f9bb5168919a3ec5e940ac01: Wormhole (2 samples)\n",
      "Pattern b03c1d546cbc6ed3c88b8a5bbd34ed7d: Unknown (11 samples)\n",
      "Pattern f954460e8ad3552bf8f294063e07e833: Wormhole (3 samples)\n",
      "Pattern 47372907632c9a76b8f7478d280f17b7: Wormhole (6 samples)\n",
      "Pattern 80c7b416be7816b5d19bbc6fc64382f3: Wormhole (2 samples)\n",
      "Pattern 526eddb09d127f2823236a91f80c3274: Wormhole (1 samples)\n",
      "Pattern 83b454e610de597f24830b357d832222: Wormhole (1 samples)\n",
      "Pattern 0d1899a31ad0df947cae71a4252ad004: Wormhole (4 samples)\n",
      "Pattern 46841ef22dcf569a7de528f75c646ed1: Wormhole (1 samples)\n",
      "Pattern 975df8ba6889b5d07d6a9e41b47b74cb: Wormhole (1 samples)\n",
      "Pattern d8cd5cf7be398bf6ec514523b180d5ce: Wormhole (2 samples)\n",
      "Pattern 0a859377c9adc6f48a0c39da47d1658a: Wormhole (1 samples)\n",
      "Pattern d0d86aa21af25221a69b3c4df5c0e23e: Wormhole (1 samples)\n",
      "Pattern c54b01ddf848e4fa3585463910669142: Wormhole (1 samples)\n",
      "Pattern 2066ec5588cd33591b66c060d642dd5a: Wormhole (1 samples)\n",
      "Pattern e283ce3e368f9836116c53ac75f3b6dc: Wormhole (2 samples)\n",
      "Pattern cf138fa45c4f02a48587dd08396acc25: Wormhole (1 samples)\n",
      "Pattern c5d667c5f3b1c889e368863eb9df0a7d: Wormhole (1 samples)\n",
      "Pattern 0081edae00a714df4d66b504dd2cb536: Wormhole (1 samples)\n",
      "Pattern 47c9a74cdb0856de0eb852669df33f4b: Wormhole (1 samples)\n",
      "Pattern 9b145b2869d31f87ef4078e39c2da7f4: Wormhole (2 samples)\n",
      "Pattern 164ca248e5cccd024c607aadacebcb85: Wormhole (3 samples)\n",
      "Pattern edfc7d351a64f610a610973e5be6b780: Wormhole (3 samples)\n",
      "Pattern 85de6fb98d5dfca12e784cb845a2a29b: Wormhole (1 samples)\n",
      "Pattern 75f54c3878adc618e188f60256ece0be: Wormhole (1 samples)\n",
      "Pattern 7f1064226815c6195b5d206fe67ce7c2: Wormhole (1 samples)\n",
      "Pattern fa26507b138cf829fbf4faa825f3dfbd: Wormhole (1 samples)\n",
      "Pattern 8bf11d74b065863e0c9c0afabe40f88f: Spinner (3 samples)\n",
      "Pattern 941324346186b34488881ad7667bb6aa: Wormhole (1 samples)\n",
      "Pattern 98099f5398a0f560f88f4884de76f61d: Wormhole (1 samples)\n",
      "Pattern fce8201a8a5105ca3b824cc7c202a9ed: Wormhole (1 samples)\n",
      "Pattern 6b8e050624a6df5428215ab098ec28e7: Wormhole (1 samples)\n",
      "Pattern 522ee13b85d67f0bd4054be6b6203646: Wormhole (1 samples)\n",
      "Pattern d60e34d1456f1b3cc0e779e8348d47d9: Wormhole (1 samples)\n",
      "Pattern d062c927659a5e3532d8f6c497671eed: Wormhole (2 samples)\n",
      "Pattern 71df167829130e98ca1ca7108b973c35: Wormhole (1 samples)\n",
      "Pattern c8d6b2cdd87dee62aac07434689ff0fa: Wormhole (2 samples)\n",
      "Pattern dc33a7b4ac887c745ea0d1b598963708: Wormhole (1 samples)\n",
      "Pattern 6c8e28f9a275ad2118a05368bd2a9b76: Wormhole (1 samples)\n",
      "Pattern 7cefe3298283d2043cbe1747c9b9dab5: Wormhole (1 samples)\n",
      "Pattern cb808e05cc36fb995b5b8c3e3decd7b7: Spinner (1 samples)\n",
      "Pattern c086e0f5ecb38629f64071772b246667: Wormhole (1 samples)\n",
      "Pattern 9a1687a32d86f5d842dda6b6018137fe: Wormhole (1 samples)\n",
      "Pattern 72e9a86ce6a5ab96d1309d0d4a047879: Wormhole (1 samples)\n",
      "Pattern 3c9adb5e7e88b144e78bc1844cb5645d: Wormhole (1 samples)\n",
      "Pattern 757b9602aa700502df60d9a12c706756: Wormhole (1 samples)\n",
      "Pattern cc98c2a7106c4cf2194ab38842f66142: Wormhole (1 samples)\n",
      "Pattern 60d325bb09e9922f211df99f801eca38: Wormhole (1 samples)\n",
      "\n",
      "====== Testing ======\n",
      "\n",
      "Total number of campaigns in the testing dataset: 435\n",
      "\n",
      "Plotting all different patterns in the Testing set: 37\n",
      "\n",
      "Weird machine classifications in Testing: 37\n",
      "Pattern 3e8ac51ed1241beac4ad7b88af311791: Tunnel (62 samples)\n",
      "Pattern a6f2e5ee1911c56a1599d39c521ac1bb: Wormhole (36 samples)\n",
      "Pattern 2dd3fd3b2c7df350241568aaf9797189: Wormhole (175 samples)\n",
      "Pattern e4a9c4ac4b84be7668569728c8f105ad: Wormhole (27 samples)\n",
      "Pattern 04520a5eef89f66e50b9002385ab47fb: Wormhole (6 samples)\n",
      "Pattern 0fe5d3e7f349b764ed0cd8eb5269291e: Wormhole (1 samples)\n",
      "Pattern 0bc05b01d66b72cc33afc1b94ec6b455: Tunnel (41 samples)\n",
      "Pattern 2fea8ba647832c23794f66fbba65b879: Wormhole (4 samples)\n",
      "Pattern 184a37a7bc6649027d7bc630f251903f: Wormhole (39 samples)\n",
      "Pattern 470d3fddd532ac2834d4f35750e68fcd: Tunnel (2 samples)\n",
      "Pattern 2b6427761951fbb64cd4748e7ce0de95: Wormhole (1 samples)\n",
      "Pattern b4b99b85f8ce7789d93465837604ab72: Tunnel (1 samples)\n",
      "Pattern d271b60396bad7e52aa166e8615bce64: Tunnel (2 samples)\n",
      "Pattern 2cd250f621eaaa3553aadb1ef87980d6: Wormhole (1 samples)\n",
      "Pattern 7e252d378eba6eaaca3c3e4c0978e0cc: Wormhole (1 samples)\n",
      "Pattern a273b295b42436604d8253eea9161961: Wormhole (2 samples)\n",
      "Pattern 08e8c8ab9455520881ab410f98fbb854: Wormhole (5 samples)\n",
      "Pattern aed40701e3fb5c960dc12c581fd9de1a: Spinner (1 samples)\n",
      "Pattern 6622bc2e2e6203d8bce93101bcfa9c1d: Spinner (4 samples)\n",
      "Pattern 8e0df7746bea792a82e3ef7495e240d7: Tunnel (3 samples)\n",
      "Pattern e283ce3e368f9836116c53ac75f3b6dc: Wormhole (1 samples)\n",
      "Pattern 393d3b3be7ba0fa72b547ea023db732e: Wormhole (1 samples)\n",
      "Pattern b0893b713f2a3315729da555c0cd2d8a: Wormhole (1 samples)\n",
      "Pattern f954460e8ad3552bf8f294063e07e833: Wormhole (1 samples)\n",
      "Pattern b03c1d546cbc6ed3c88b8a5bbd34ed7d: Wormhole (2 samples)\n",
      "Pattern a8f674f3aea614cd37c2bc890663c47f: Wormhole (1 samples)\n",
      "Pattern cc98c2a7106c4cf2194ab38842f66142: Wormhole (1 samples)\n",
      "Pattern 8bf11d74b065863e0c9c0afabe40f88f: Spinner (2 samples)\n",
      "Pattern 6c8e28f9a275ad2118a05368bd2a9b76: Wormhole (1 samples)\n",
      "Pattern d6e6bb9b0793787defe276d48d963d66: Wormhole (1 samples)\n",
      "Pattern 3e619b6c5540ca1e9ce0686641558fd8: Wormhole (2 samples)\n",
      "Pattern c894b456887d2ab95ef6ba2b5eab010c: Wormhole (1 samples)\n",
      "Pattern 75f54c3878adc618e188f60256ece0be: Wormhole (1 samples)\n",
      "Pattern 790523bcdfc630dfc7f7a35273a8275b: Wormhole (2 samples)\n",
      "Pattern ad2abe534c63ea9b3db9da08da4ecbe7: Wormhole (1 samples)\n",
      "Pattern 1501a451efb1ea58cc45542d87061b07: Wormhole (1 samples)\n",
      "Pattern 0081edae00a714df4d66b504dd2cb536: Wormhole (1 samples)\n",
      "=========== Result =========\n",
      "\n",
      "Number of weird machine patterns in the testing dataset already seen in the training dataset: 26\n",
      "Number of weird machine patterns in the testing dataset not seen in the training dataset: 11\n",
      "New weird machine pattern(s) in the testing dataset: ['0fe5d3e7f349b764ed0cd8eb5269291e', '2b6427761951fbb64cd4748e7ce0de95', 'b4b99b85f8ce7789d93465837604ab72', '2cd250f621eaaa3553aadb1ef87980d6', 'a273b295b42436604d8253eea9161961', '393d3b3be7ba0fa72b547ea023db732e', 'b0893b713f2a3315729da555c0cd2d8a', 'a8f674f3aea614cd37c2bc890663c47f', 'c894b456887d2ab95ef6ba2b5eab010c', 'ad2abe534c63ea9b3db9da08da4ecbe7', '1501a451efb1ea58cc45542d87061b07']\n",
      "New weird machine pattern(s) in the testing dataset dot file(s): ['/media/dillibabu/PortableSSD/Exp/capri6_rootcause_data/lasersweep/lasersweep_verifypin5/Pinverify5/testing_dotfiles/tb-2863_2356_core3_uarchi_abstract.dot', '/media/dillibabu/PortableSSD/Exp/capri6_rootcause_data/lasersweep/lasersweep_verifypin5/Pinverify5/testing_dotfiles/tb-2983_2566_core3_uarchi_abstract.dot', '/media/dillibabu/PortableSSD/Exp/capri6_rootcause_data/lasersweep/lasersweep_verifypin5/Pinverify5/testing_dotfiles/tb-2803_2251_core3_uarchi_abstract.dot', '/media/dillibabu/PortableSSD/Exp/capri6_rootcause_data/lasersweep/lasersweep_verifypin5/Pinverify5/testing_dotfiles/tb-2998_2536_core3_uarchi_abstract.dot', '/media/dillibabu/PortableSSD/Exp/capri6_rootcause_data/lasersweep/lasersweep_verifypin5/Pinverify5/testing_dotfiles/tb-2773_2461_core3_uarchi_abstract.dot', '/media/dillibabu/PortableSSD/Exp/capri6_rootcause_data/lasersweep/lasersweep_verifypin5/Pinverify5/testing_dotfiles/tb-2608_2431_core3_uarchi_abstract.dot', '/media/dillibabu/PortableSSD/Exp/capri6_rootcause_data/lasersweep/lasersweep_verifypin5/Pinverify5/testing_dotfiles/tb-2668_2296_core3_uarchi_abstract.dot', '/media/dillibabu/PortableSSD/Exp/capri6_rootcause_data/lasersweep/lasersweep_verifypin5/Pinverify5/testing_dotfiles/tb-2743_2416_core3_uarchi_abstract.dot', '/media/dillibabu/PortableSSD/Exp/capri6_rootcause_data/lasersweep/lasersweep_verifypin5/Pinverify5/testing_dotfiles/tb-2683_2431_core3_uarchi_abstract.dot', '/media/dillibabu/PortableSSD/Exp/capri6_rootcause_data/lasersweep/lasersweep_verifypin5/Pinverify5/testing_dotfiles/tb-2743_2386_core3_uarchi_abstract.dot', '/media/dillibabu/PortableSSD/Exp/capri6_rootcause_data/lasersweep/lasersweep_verifypin5/Pinverify5/testing_dotfiles/tb-2863_2551_core3_uarchi_abstract.dot', '/media/dillibabu/PortableSSD/Exp/capri6_rootcause_data/lasersweep/lasersweep_verifypin5/Pinverify5/testing_dotfiles/tb-2878_2086_core3_uarchi_abstract.dot']\n"
     ]
    }
   ],
   "source": [
    "#import torch\n",
    "#import torch.nn.functional as F\n",
    "#from torch_geometric.nn import GATConv, SAGEConv\n",
    "#from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "import pygraphviz as pgv\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from hashlib import md5\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from networkx.algorithms.isomorphism import GraphMatcher\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import shutil\n",
    "# Getting the current date and time\n",
    "dt = datetime.now()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    return ''.join(BeautifulSoup(text, \"html.parser\").stripped_strings)\n",
    "\n",
    "# Function to convert clock cycle labels (e.g., \"@21+41k\" to 62)\n",
    "def convert_clock_cycle(label):\n",
    "    cleaned_label = strip_html_tags(label)\n",
    "    match = re.match(r'@(\\d+)(\\+(\\d+)k)?', cleaned_label)\n",
    "    if match:\n",
    "        base = int(match.group(1))\n",
    "        increment = int(match.group(3)) * 1 if match.group(3) else 0\n",
    "        return base + increment\n",
    "    return 0\n",
    "\n",
    "# Function to parse .dot file and prepare the graph\n",
    "def parse_dot_file(dot_path):\n",
    "    A = pgv.AGraph(file=dot_path)\n",
    "    G = nx.DiGraph(A)\n",
    "    \n",
    "    for u, v, data in G.edges(data=True):\n",
    "        if 'label' in data:\n",
    "            data['label'] = convert_clock_cycle(data['label'])\n",
    "    \n",
    "    return G\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    # Replace invalid characters with underscores\n",
    "    return re.sub(r'[<>:\"/\\\\|?*]', '_', filename)\n",
    "\n",
    "# Function to hash a knowledge graph to identify unique patterns\n",
    "def hash_graph(G):\n",
    "    graph_str = nx.weisfeiler_lehman_graph_hash(G)\n",
    "    return md5(graph_str.encode('utf-8')).hexdigest()\n",
    "\n",
    "def plot_graph(G, title, highlight_edges=None):\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    nx.draw(G, pos, with_labels=True, node_size=500, node_color='lightblue', edge_color='gray', font_size=12, font_weight='bold')\n",
    "    \n",
    "    # Adding edge labels\n",
    "    edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='red', label_pos=0.3, bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))\n",
    "    \n",
    "    # Highlight the edges/nodes in red if they are in the highlight list\n",
    "    if highlight_edges:\n",
    "        nx.draw_networkx_edges(G, pos, edgelist=highlight_edges, edge_color='red', width=2.5)\n",
    "\n",
    "    plt.title(title)\n",
    "    \n",
    "    # Sanitize title to create a valid filename\n",
    "    safe_title = sanitize_filename(title)\n",
    "    numeric_plot_filename = f'{safe_title}.png'\n",
    "    \n",
    "    plt.savefig(numeric_plot_filename)\n",
    "    plt.close()\n",
    "# Function to create a knowledge graph representation\n",
    "def create_knowledge_graph(G):\n",
    "    knowledge_graph = nx.Graph()\n",
    "\n",
    "    # Add nodes with features as attributes\n",
    "    for node in G.nodes():\n",
    "        knowledge_graph.add_node(node, features=G.nodes[node])\n",
    "\n",
    "    # Add edges with attributes\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        knowledge_graph.add_edge(u, v, **data)\n",
    "\n",
    "    return knowledge_graph\n",
    "\n",
    "# Function to classify a pattern based on the rules provided\n",
    "def classify_pattern(G):\n",
    "    first_five_nodes = set([f\"M{i}\" for i in range(4)])  # Assuming nodes are named like M0, M1, M2, M3,\n",
    "    long_jump_found = False\n",
    "    long_jump_node = None\n",
    "    pattern_type = \"Unknown\"\n",
    "\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        try:\n",
    "            u_index = int(re.findall(r'\\d+', u)[0])\n",
    "            v_index = int(re.findall(r'\\d+', v)[0])\n",
    "        except (IndexError, ValueError):\n",
    "            continue\n",
    "\n",
    "        if abs(u_index - v_index) >= 2:\n",
    "            long_jump_found = True\n",
    "            long_jump_node = v\n",
    "            break\n",
    "\n",
    "    if long_jump_found:\n",
    "        for node in nx.dfs_postorder_nodes(G, source=long_jump_node):\n",
    "            if G.has_edge(node, node):\n",
    "                pattern_type = \"Spinner\"\n",
    "                break\n",
    "        else:\n",
    "            if long_jump_node in first_five_nodes:\n",
    "                connects_back = False\n",
    "                for node in nx.dfs_postorder_nodes(G, source=long_jump_node):\n",
    "                    for u, v in G.edges(data=False):\n",
    "                        if u == node and v in first_five_nodes:\n",
    "                            connects_back = True\n",
    "                            break\n",
    "                    if connects_back:\n",
    "                        break\n",
    "                if not connects_back:\n",
    "                    pattern_type = \"Tunnel and Wormhole\"\n",
    "                else:\n",
    "                    pattern_type = \"Unknown\"\n",
    "            else:\n",
    "                pattern_type = \"Wormhole\"\n",
    "    else:\n",
    "        for u, v, data in G.edges(data=True):\n",
    "            if u in first_five_nodes and v in first_five_nodes:\n",
    "                pattern_type = \"Tunnel\"\n",
    "                break\n",
    "\n",
    "    return pattern_type\n",
    "\n",
    "# Base folder path\n",
    "#base_folder = \"/mnt/labdrive/zliu12/capri6_rootcause_data/lasersweep/lasersweep_asconsbox_bitslice/\"\n",
    "base_folder = \"/media/dillibabu/PortableSSD/Exp/capri6_rootcause_data/lasersweep/lasersweep_verifypin5/Pinverify5/\"\n",
    "training_dotfiles_folder_path = os.path.join(base_folder, 'training_dotfiles') \n",
    "testing_dotfiles_folder_path = os.path.join(base_folder, 'testing_dotfiles') \n",
    "# Function to strip HTML tags from labels (if present)\n",
    "def strip_html_tags(text):\n",
    "    return ''.join(BeautifulSoup(text, \"html.parser\").stripped_strings)\n",
    "\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "core0=[]\n",
    "core1=[]\n",
    "core2=[]\n",
    "core3=[]\n",
    "core4=[]\n",
    "core5=[]\n",
    "\n",
    "\n",
    "\n",
    "def gather_and_classify_training_dot_files(training_dotfiles_folder_path, exclude_folders=['coremajority']):\n",
    "    dot_files = defaultdict(list)  # To store files by categories\n",
    "    categories_count = defaultdict(int)  # To count files in each category\n",
    "    training_dotfiles= []\n",
    "\n",
    "    for dot_file in os.listdir(training_dotfiles_folder_path):\n",
    "        if dot_file.endswith('uarchi_abstract.dot') and not any(exclude in dot_file for exclude in exclude_folders) and 'core3' not in dot_file:\n",
    "            training_dotfiles.append(os.path.join(training_dotfiles_folder_path, dot_file))\n",
    "#                         print('dot_file_training:',dot_file)\n",
    "\n",
    "            category = None\n",
    "            if 'core0' in dot_file:\n",
    "                category = 'core0'\n",
    "                core0.append(dot_file)\n",
    "            elif 'core1' in dot_file:\n",
    "                category = 'core1'\n",
    "                core1.append(dot_file)\n",
    "            elif 'core2' in dot_file:\n",
    "                category = 'core2'\n",
    "                core2.append(dot_file)\n",
    "            elif 'core3' in dot_file:\n",
    "                category = 'core3'\n",
    "                core3.append(dot_file)\n",
    "            elif 'core4' in dot_file:\n",
    "                category = 'core4'\n",
    "                core4.append(dot_file)\n",
    "            elif 'core5' in dot_file:\n",
    "                category = 'core5'\n",
    "                core5.append(dot_file)\n",
    "\n",
    "            # Add more elif cases for other cores if needed\n",
    "\n",
    "            if category:\n",
    "                dot_files[category].append(os.path.join(training_dotfiles_folder_path, dot_file))\n",
    "                categories_count[category] += 1\n",
    "                    \n",
    "                    \n",
    "\n",
    "    # Print the total number of .dot files in each category\n",
    "    for category, files in dot_files.items():\n",
    "        print(f'Category: {category}, Number of .dot files: {categories_count[category]}')\n",
    "#     print('core0',core0)\n",
    "#     print('core1',core1)\n",
    "#     print('core2',core2)\n",
    "#     print('core4',core4)\n",
    "#     print('core5',core5)\n",
    "\n",
    "    return training_dotfiles,dot_files, categories_count\n",
    "\n",
    "\n",
    "def gather_and_classify_testing_dot_files(testing_dotfiles_folder_path, exclude_folders=['coremajority']):\n",
    "    dot_files = defaultdict(list)  # To store files by categories\n",
    "    categories_count = defaultdict(int)  # To count files in each category\n",
    "    testing_dotfiles=[]\n",
    "    \n",
    "    for dot_file in os.listdir(testing_dotfiles_folder_path):\n",
    "        if dot_file.endswith('uarchi_abstract.dot') and not any(exclude in dot_file for exclude in exclude_folders) and 'core3' in dot_file:\n",
    "            testing_dotfiles.append(os.path.join(testing_dotfiles_folder_path, dot_file))\n",
    "#                         print('dot_file_training:',dot_file)\n",
    "\n",
    "            category = None\n",
    "            if 'core3' in dot_file:\n",
    "                category = 'core3'\n",
    "                core3.append(dot_file)\n",
    "            if category:\n",
    "                dot_files[category].append(os.path.join(testing_dotfiles_folder_path, dot_file))\n",
    "                categories_count[category] += 1\n",
    "                    \n",
    "    # Print the total number of .dot files in each category\n",
    "    for category, files in dot_files.items():\n",
    "        print(f'Category: {category}, Number of .dot files: {categories_count[category]}')\n",
    "#     print('core3',core3)\n",
    "    \n",
    "\n",
    "    return testing_dotfiles,dot_files, categories_count\n",
    "\n",
    "\n",
    "training_dot_file_paths,  category_type, categories_count = gather_and_classify_training_dot_files(training_dotfiles_folder_path)\n",
    "\n",
    "testing_dot_file_paths, category_type, categories_count = gather_and_classify_testing_dot_files(testing_dotfiles_folder_path)\n",
    "#-----------------------\n",
    "\n",
    "# Check if tb_13cc_dot_file_paths is empty\n",
    "if not testing_dot_file_paths:\n",
    "    print(\"No .dot files found in the testing directory. Please check the directory path.\")\n",
    "else:\n",
    "    # Load the list from the file\n",
    "    #with open('train_dot_file_paths.pkl', 'rb') as file:\n",
    "        #train_dot_file_paths = pickle.load(file)\n",
    "    # Split tb-13cc data into training and testing sets\n",
    "    tb_cc_test_files = testing_dot_file_paths\n",
    "\n",
    "    # Parse and prepare training knowledge graphs and collect patterns\n",
    "    train_patterns = {}\n",
    "    pattern_count_train = {}\n",
    "    pattern_graphs_train = {}\n",
    "    pattern_classification_train = {}\n",
    "    for path in training_dot_file_paths:\n",
    "        train_G = parse_dot_file(path)\n",
    "        train_kg = create_knowledge_graph(train_G)\n",
    "        pattern_hash = hash_graph(train_kg)\n",
    "        train_patterns[pattern_hash] = train_G\n",
    "        pattern_count_train[pattern_hash] = pattern_count_train.get(pattern_hash, 0) + 1\n",
    "        if pattern_hash not in pattern_graphs_train:\n",
    "            pattern_graphs_train[pattern_hash] = []\n",
    "        pattern_graphs_train[pattern_hash].append(train_G)\n",
    "        pattern_type = classify_pattern(train_G)\n",
    "        pattern_classification_train[pattern_hash] = pattern_type\n",
    "\n",
    "    # Parse and prepare testing knowledge graphs and collect patterns\n",
    "    test_patterns = {}\n",
    "    pattern_count_test = {}\n",
    "    pattern_graphs_test = {}\n",
    "    pattern_classification_test = {}\n",
    "    seen_in_training = 0\n",
    "    not_seen_in_training = 0\n",
    "    unknown_pattern_dot_files = []\n",
    "    known_pattern_dot_files = []\n",
    "    new_pattern_dot_files = []\n",
    "\n",
    "    for path in tb_cc_test_files:\n",
    "        test_G = parse_dot_file(path)\n",
    "        test_kg = create_knowledge_graph(test_G)\n",
    "        pattern_hash = hash_graph(test_kg)\n",
    "        test_patterns[pattern_hash] = test_G\n",
    "        pattern_count_test[pattern_hash] = pattern_count_test.get(pattern_hash, 0) + 1\n",
    "        if pattern_hash not in pattern_graphs_test:\n",
    "            pattern_graphs_test[pattern_hash] = []\n",
    "        pattern_graphs_test[pattern_hash].append(test_G)\n",
    "        pattern_type = classify_pattern(test_G)\n",
    "        pattern_classification_test[pattern_hash] = pattern_type\n",
    "        \n",
    "        if pattern_hash in train_patterns:\n",
    "            seen_in_training += 1\n",
    "            plot_graph(test_G, title=f\"Test Pattern Seen in Training: {pattern_hash}\", highlight_edges=list(test_G.edges()))\n",
    "        else:\n",
    "            not_seen_in_training += 1\n",
    "            plot_graph(test_G, title=f\"Test Pattern Not Seen in Training: {pattern_hash}\", highlight_edges=list(test_G.edges()))\n",
    "            new_pattern_dot_files.append(path)\n",
    "    \n",
    "    unknown_patterns = []\n",
    "    known_patterns = []\n",
    "    no_unknown_pattern = 0\n",
    "    no_known_pattern = 0\n",
    "\n",
    "    for pattern_hash, pattern_type in pattern_classification_test.items():\n",
    "        #print('pattern_hash', pattern_hash)\n",
    "        if pattern_hash not in pattern_classification_train:\n",
    "            unknown_patterns.append(pattern_hash)\n",
    "            no_unknown_pattern += 1\n",
    "        else:\n",
    "            known_patterns.append(pattern_hash)\n",
    "            no_known_pattern += 1\n",
    "        \n",
    "\n",
    "\n",
    "    total_train_samples = sum(pattern_count_train.values())\n",
    "    total_test_samples = sum(pattern_count_test.values())\n",
    "    \n",
    "    print(\"\\n====== Training ======\\n\")\n",
    "    print(f\"Total number of campaigns in the training dataset: {total_train_samples}\")\n",
    "    # Plot all different patterns found in the training set\n",
    "    print(\"\\nPlotting all different patterns in the Training set:\", len(pattern_classification_train.items()))\n",
    "    for pattern_hash, G in train_patterns.items():\n",
    "        plot_graph(G, title=f\"Training Pattern: {pattern_hash} ({pattern_classification_train[pattern_hash]})\")\n",
    "\n",
    "    # Output the classification results\n",
    "    print(\"\\nWeird Machine classifications in Training:\", len(pattern_classification_train.items()))\n",
    "    for pattern_hash, pattern_type in pattern_classification_train.items():\n",
    "        print(f\"Pattern {pattern_hash}: {pattern_type} ({pattern_count_train[pattern_hash]} samples)\")\n",
    "    print(\"\\n====== Testing ======\\n\")\n",
    "    print(f\"Total number of campaigns in the testing dataset: {total_test_samples}\")   \n",
    "    \n",
    "    \n",
    "    # Plot all different patterns found in the testing set\n",
    "    print(\"\\nPlotting all different patterns in the Testing set:\", len(pattern_classification_test.items()))\n",
    "    for pattern_hash, G in test_patterns.items():\n",
    "        plot_graph(G, title=f\"Testing Pattern: {pattern_hash} ({pattern_classification_test[pattern_hash]})\")\n",
    "\n",
    "    \n",
    "    print(\"\\nWeird machine classifications in Testing:\", len(pattern_classification_test.items()))\n",
    "    for pattern_hash, pattern_type in pattern_classification_test.items():\n",
    "        print(f\"Pattern {pattern_hash}: {pattern_type} ({pattern_count_test[pattern_hash]} samples)\")\n",
    "\n",
    "    print(\"=========== Result =========\\n\")\n",
    "    print(f\"Number of weird machine patterns in the testing dataset already seen in the training dataset: {no_known_pattern}\")\n",
    "    print(f\"Number of weird machine patterns in the testing dataset not seen in the training dataset: {no_unknown_pattern}\")\n",
    "    print(f\"New weird machine pattern(s) in the testing dataset: {unknown_patterns}\")\n",
    "    print(f\"New weird machine pattern(s) in the testing dataset dot file(s): {new_pattern_dot_files}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369777f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
